{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-19 17:48:18.840810: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-19 17:48:18.939395: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-19 17:48:19.304692: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-19 17:48:19.304731: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-19 17:48:19.304734: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "# from ...activations import ACT2FN, gelu\n",
    "from transformers.activations import ACT2FN, gelu\n",
    "# from ...modeling_outputs import (\n",
    "#     BaseModelOutputWithPastAndCrossAttentions,\n",
    "#     BaseModelOutputWithPoolingAndCrossAttentions,\n",
    "#     CausalLMOutputWithCrossAttentions,\n",
    "#     MaskedLMOutput,\n",
    "#     MultipleChoiceModelOutput,\n",
    "#     QuestionAnsweringModelOutput,\n",
    "#     SequenceClassifierOutput,\n",
    "#     TokenClassifierOutput,\n",
    "# )\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    BaseModelOutputWithPoolingAndCrossAttentions,\n",
    "    CausalLMOutputWithCrossAttentions,\n",
    "    MaskedLMOutput,\n",
    "    MultipleChoiceModelOutput,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutput,\n",
    "    TokenClassifierOutput,\n",
    ")\n",
    "# from ...modeling_utils import PreTrainedModel\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "# from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n",
    "from transformers.pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n",
    "# from ...utils import (\n",
    "#     add_code_sample_docstrings,\n",
    "#     add_start_docstrings,\n",
    "#     add_start_docstrings_to_model_forward,\n",
    "#     logging,\n",
    "#     replace_return_docstrings,\n",
    "# )\n",
    "from transformers.utils import (\n",
    "    add_code_sample_docstrings,\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "# from .configuration_roberta import RobertaConfig\n",
    "from transformers.models.roberta.configuration_roberta import RobertaConfig\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "_CHECKPOINT_FOR_DOC = \"roberta-base\"\n",
    "_CONFIG_FOR_DOC = \"RobertaConfig\"\n",
    "\n",
    "ROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
    "    \"roberta-base\",\n",
    "    \"roberta-large\",\n",
    "    \"roberta-large-mnli\",\n",
    "    \"distilroberta-base\",\n",
    "    \"roberta-base-openai-detector\",\n",
    "    \"roberta-large-openai-detector\",\n",
    "    # See all RoBERTa models at https://huggingface.co/models?filter=roberta\n",
    "]\n",
    "\n",
    "ROBERTA_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        input_ids (`torch.LongTensor` of shape `({0})`):\n",
    "            Indices of input sequence tokens in the vocabulary.\n",
    "\n",
    "            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
    "            [`PreTrainedTokenizer.__call__`] for details.\n",
    "\n",
    "            [What are input IDs?](../glossary#input-ids)\n",
    "        attention_mask (`torch.FloatTensor` of shape `({0})`, *optional*):\n",
    "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "\n",
    "            [What are attention masks?](../glossary#attention-mask)\n",
    "        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n",
    "            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n",
    "\n",
    "            - 0 corresponds to a *sentence A* token,\n",
    "            - 1 corresponds to a *sentence B* token.\n",
    "            This parameter can only be used when the model is initialized with `type_vocab_size` parameter with value\n",
    "            >= 2. All the value in this tensor should be always < type_vocab_size.\n",
    "\n",
    "            [What are token type IDs?](../glossary#token-type-ids)\n",
    "        position_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n",
    "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n",
    "            config.max_position_embeddings - 1]`.\n",
    "\n",
    "            [What are position IDs?](../glossary#position-ids)\n",
    "        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n",
    "            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 indicates the head is **not masked**,\n",
    "            - 0 indicates the head is **masked**.\n",
    "\n",
    "        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n",
    "            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
    "            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n",
    "            model's internal embedding lookup matrix.\n",
    "        output_attentions (`bool`, *optional*):\n",
    "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
    "            tensors for more detail.\n",
    "        output_hidden_states (`bool`, *optional*):\n",
    "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
    "            more detail.\n",
    "        return_dict (`bool`, *optional*):\n",
    "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.roberta.modeling_roberta import (\n",
    "    RobertaAttention, \n",
    "    RobertaPreTrainedModel, \n",
    "    RobertaPooler, \n",
    "    RobertaEmbeddings, \n",
    "    RobertaClassificationHead\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(1.702 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Boom_Layer(nn.Module):\n",
    "     def __init__(self, in_features: int, out_features: int, dropout=0.1, shortcut: bool = True, device=None, dtype=None) -> None:\n",
    "         factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "         super(Boom_Layer, self).__init__()\n",
    "\n",
    "         self.linear1 = nn.Linear(in_features, out_features)\n",
    "         self.dropout = nn.Dropout(dropout) if dropout else None\n",
    "         if not shortcut:\n",
    "             self.linear2 = nn.Linear(out_features, in_features)\n",
    "         self.shortcut = shortcut\n",
    "         self.act = GELU()\n",
    " \n",
    "     def forward(self, input: Tensor) -> Tensor:\n",
    "         x = self.act(self.linear1(input))\n",
    "         if self.dropout: x = self.dropout(x)\n",
    "         if self.shortcut:\n",
    "             ninp = input.shape[-1]\n",
    "             x = torch.narrow(x, -1, 0, x.shape[-1] // ninp * ninp)\n",
    "             x = x.view(*x.shape[:-1], x.shape[-1] // ninp, ninp)\n",
    "             z = x.sum(dim=-2)\n",
    "         else:\n",
    "             z = self.linear2(x)\n",
    " \n",
    "         return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.boom = Boom_Layer(config.hidden_size, (config.hidden_size * BOOM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaIntermediateAndBoom(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # self.dense_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        # if isinstance(config.hidden_act, str):\n",
    "        #     self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
    "        # else:\n",
    "        #     self.intermediate_act_fn = config.hidden_act\n",
    "\n",
    "        # self.dense_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "\n",
    "        self.dense_1 = nn.Linear(config.hidden_size, config.hidden_size/2)\n",
    "        self.dense_2 = nn.Linear(config.hidden_size/2, config.hidden_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        # hidden_states = self.dense_1(input_tensor)\n",
    "        # hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "\n",
    "        # hidden_states = self.dense_2(hidden_states)\n",
    "\n",
    "        hidden_states = self.dense_1(input_tensor)\n",
    "        hidden_states = self.dense_2(hidden_states)\n",
    "\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedRobertaLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        self.attention = RobertaAttention(config)\n",
    "        self.is_decoder = config.is_decoder\n",
    "        self.add_cross_attention = config.add_cross_attention\n",
    "        if self.add_cross_attention:\n",
    "            if not self.is_decoder:\n",
    "                raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n",
    "            self.crossattention = RobertaAttention(config, position_embedding_type=\"absolute\")\n",
    "        # self.intermediate = RobertaIntermediate(config)#####\n",
    "        # self.output = RobertaOutput(config)#####\n",
    "        \n",
    "        self.intermediateandboom = RobertaIntermediateAndBoom(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
    "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
    "        self_attention_outputs = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            past_key_value=self_attn_past_key_value,\n",
    "        )\n",
    "        attention_output = self_attention_outputs[0]\n",
    "\n",
    "        # if decoder, the last output is tuple of self-attn cache\n",
    "        if self.is_decoder:\n",
    "            outputs = self_attention_outputs[1:-1]\n",
    "            present_key_value = self_attention_outputs[-1]\n",
    "        else:\n",
    "            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "        cross_attn_present_key_value = None\n",
    "        if self.is_decoder and encoder_hidden_states is not None:\n",
    "            if not hasattr(self, \"crossattention\"):\n",
    "                raise ValueError(\n",
    "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers\"\n",
    "                    \" by setting `config.add_cross_attention=True`\"\n",
    "                )\n",
    "\n",
    "            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n",
    "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
    "            cross_attention_outputs = self.crossattention(\n",
    "                attention_output,\n",
    "                attention_mask,\n",
    "                head_mask,\n",
    "                encoder_hidden_states,\n",
    "                encoder_attention_mask,\n",
    "                cross_attn_past_key_value,\n",
    "                output_attentions,\n",
    "            )\n",
    "            attention_output = cross_attention_outputs[0]\n",
    "            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n",
    "\n",
    "            # add cross-attn cache to positions 3,4 of present_key_value tuple\n",
    "            cross_attn_present_key_value = cross_attention_outputs[-1]\n",
    "            present_key_value = present_key_value + cross_attn_present_key_value\n",
    "\n",
    "        layer_output = apply_chunking_to_forward(\n",
    "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
    "        )\n",
    "        outputs = (layer_output,) + outputs\n",
    "\n",
    "        # if decoder, return the attn key/values as the last output\n",
    "        if self.is_decoder:\n",
    "            outputs = outputs + (present_key_value,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def feed_forward_chunk(self, attention_output):\n",
    "        # print(attention_output.shape)\n",
    "        # intermediate_output = self.intermediate(attention_output)\n",
    "        # print(intermediate_output.shape)\n",
    "        # layer_output = self.output(intermediate_output, attention_output)\n",
    "        # print(layer_output.shape)\n",
    "\n",
    "        layer_output = self.intermediateandboom(attention_output)\n",
    "\n",
    "        return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedRobertaEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = nn.ModuleList([ModifiedRobertaLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        output_hidden_states: Optional[bool] = False,\n",
    "        return_dict: Optional[bool] = True,\n",
    "    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                if use_cache:\n",
    "                    logger.warning(\n",
    "                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                    )\n",
    "                    use_cache = False\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(layer_module),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    past_key_value,\n",
    "                    output_attentions,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[-1],)\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedRobertaModel(RobertaPreTrainedModel):\n",
    "    \"\"\"\n",
    "\n",
    "    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
    "    cross-attention is added between the self-attention layers, following the architecture described in *Attention is\n",
    "    all you need*_ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n",
    "    Kaiser and Illia Polosukhin.\n",
    "\n",
    "    To behave as an decoder the model needs to be initialized with the `is_decoder` argument of the configuration set\n",
    "    to `True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and\n",
    "    `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n",
    "\n",
    "    .. _*Attention is all you need*: https://arxiv.org/abs/1706.03762\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "    # Copied from transformers.models.bert.modeling_bert.BertModel.__init__ with Bert->Roberta\n",
    "    def __init__(self, config, add_pooling_layer=True):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = RobertaEmbeddings(config)\n",
    "        self.encoder = ModifiedRobertaEncoder(config)\n",
    "\n",
    "        self.pooler = RobertaPooler(config) if add_pooling_layer else None\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings.word_embeddings\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embeddings.word_embeddings = value\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\"\n",
    "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
    "        class PreTrainedModel\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "    # Copied from transformers.models.bert.modeling_bert.BertModel.forward\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n",
    "        r\"\"\"\n",
    "        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
    "            the model is configured as a decoder.\n",
    "        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
    "            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
    "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
    "\n",
    "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
    "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
    "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
    "        use_cache (`bool`, *optional*):\n",
    "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
    "            `past_key_values`).\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if self.config.is_decoder:\n",
    "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        else:\n",
    "            use_cache = False\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        batch_size, seq_length = input_shape\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        # past_key_values_length\n",
    "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            if hasattr(self.embeddings, \"token_type_ids\"):\n",
    "                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n",
    "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
    "                token_type_ids = buffered_token_type_ids_expanded\n",
    "            else:\n",
    "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n",
    "\n",
    "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
    "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
    "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            if encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "        else:\n",
    "            encoder_extended_attention_mask = None\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=position_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            past_key_values_length=past_key_values_length,\n",
    "        )\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_extended_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            past_key_values=encoder_outputs.past_key_values,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "            cross_attentions=encoder_outputs.cross_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedRobertaForSequenceClassification(RobertaPreTrainedModel):\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        self.roberta = ModifiedRobertaModel(config, add_pooling_layer=False)\n",
    "        self.classifier = RobertaClassificationHead(config)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        checkpoint=\"cardiffnlp/twitter-roberta-base-emotion\",\n",
    "        output_type=SequenceClassifierOutput,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "        expected_output=\"'optimism'\",\n",
    "        expected_loss=0.08,\n",
    "    )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_summary(model):\n",
    "    total_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            total_params += param.numel()\n",
    "    print(f\"Total Trainable Parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing ModifiedRobertaForSequenceClassification: ['roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.16.output.dense.weight', 'roberta.encoder.layer.12.output.dense.weight', 'roberta.encoder.layer.18.output.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.14.intermediate.dense.weight', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.13.output.dense.weight', 'roberta.encoder.layer.20.output.LayerNorm.weight', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.22.output.LayerNorm.bias', 'roberta.encoder.layer.22.output.dense.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.16.output.LayerNorm.bias', 'roberta.encoder.layer.16.output.dense.bias', 'roberta.encoder.layer.17.output.LayerNorm.weight', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.21.intermediate.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.23.intermediate.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.23.output.LayerNorm.bias', 'roberta.encoder.layer.15.intermediate.dense.weight', 'roberta.encoder.layer.19.intermediate.dense.weight', 'roberta.encoder.layer.13.intermediate.dense.bias', 'roberta.encoder.layer.23.output.LayerNorm.weight', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.12.intermediate.dense.bias', 'roberta.encoder.layer.17.output.dense.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.13.output.dense.bias', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.20.intermediate.dense.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.14.output.dense.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.17.intermediate.dense.bias', 'lm_head.bias', 'roberta.encoder.layer.15.output.LayerNorm.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.23.output.dense.weight', 'roberta.encoder.layer.12.output.dense.bias', 'roberta.encoder.layer.16.output.LayerNorm.weight', 'roberta.encoder.layer.14.output.LayerNorm.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.19.output.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.12.intermediate.dense.weight', 'lm_head.dense.bias', 'roberta.encoder.layer.15.intermediate.dense.bias', 'roberta.pooler.dense.weight', 'roberta.encoder.layer.22.intermediate.dense.weight', 'roberta.encoder.layer.14.output.dense.bias', 'roberta.encoder.layer.13.output.LayerNorm.bias', 'roberta.encoder.layer.17.intermediate.dense.weight', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.15.output.dense.weight', 'roberta.encoder.layer.19.output.dense.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.14.intermediate.dense.bias', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.20.output.dense.bias', 'roberta.encoder.layer.16.intermediate.dense.bias', 'roberta.encoder.layer.22.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.18.intermediate.dense.bias', 'roberta.encoder.layer.21.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.15.output.LayerNorm.bias', 'roberta.encoder.layer.17.output.LayerNorm.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.22.intermediate.dense.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.13.intermediate.dense.weight', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.18.output.LayerNorm.bias', 'roberta.encoder.layer.21.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.18.intermediate.dense.weight', 'roberta.encoder.layer.21.output.dense.weight', 'roberta.encoder.layer.21.intermediate.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.20.intermediate.dense.weight', 'roberta.encoder.layer.23.intermediate.dense.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.22.output.dense.bias', 'roberta.encoder.layer.13.output.LayerNorm.weight', 'roberta.encoder.layer.12.output.LayerNorm.weight', 'roberta.encoder.layer.21.output.dense.bias', 'roberta.encoder.layer.20.output.dense.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.19.intermediate.dense.bias', 'roberta.encoder.layer.18.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.17.output.dense.bias', 'lm_head.decoder.weight', 'roberta.encoder.layer.12.output.LayerNorm.bias', 'roberta.encoder.layer.23.output.dense.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.15.output.dense.bias', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.16.intermediate.dense.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.18.output.dense.bias', 'roberta.encoder.layer.14.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.19.output.LayerNorm.bias', 'roberta.encoder.layer.19.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.20.output.LayerNorm.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing ModifiedRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ModifiedRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ModifiedRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.encoder.layer.11.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.23.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.21.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.15.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.10.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.16.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.5.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.10.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.4.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.15.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.18.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.5.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.3.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.19.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.19.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.13.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.14.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.17.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.3.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.22.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.10.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.1.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.15.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.8.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.5.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.21.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.7.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.18.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.12.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.8.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.9.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.11.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.11.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.0.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.5.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.1.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.3.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.21.intermediateandboom.LayerNorm.bias', 'classifier.out_proj.bias', 'roberta.encoder.layer.20.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.22.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.1.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.14.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.10.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.23.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.6.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.7.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.0.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.3.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.23.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.14.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.13.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.6.intermediateandboom.boom.linear1.bias', 'classifier.dense.weight', 'roberta.encoder.layer.6.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.17.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.7.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.21.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.12.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.17.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.4.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.12.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.9.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.4.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.2.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.15.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.16.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.2.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.17.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.12.intermediateandboom.boom.linear1.weight', 'classifier.dense.bias', 'roberta.encoder.layer.6.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.8.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.20.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.18.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.7.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.14.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.22.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.13.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.19.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.2.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.9.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.1.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.11.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.8.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.20.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.20.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.22.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.18.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.4.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.9.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.2.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.0.intermediateandboom.LayerNorm.bias', 'classifier.out_proj.weight', 'roberta.encoder.layer.16.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.23.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.13.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.16.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.0.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.19.intermediateandboom.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
    "model = ModifiedRobertaForSequenceClassification.from_pretrained(\"roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.roberta.modeling_roberta import RobertaForSequenceClassification\n",
    "\n",
    "model_ = RobertaForSequenceClassification.from_pretrained(\"roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Parameters: 355361794\n"
     ]
    }
   ],
   "source": [
    "print_model_summary(model_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Parameters: 254673922\n"
     ]
    }
   ],
   "source": [
    "print_model_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.roberta.embeddings.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for i in range(len(model.roberta.encoder.layer)):\n",
    "    for param in model.roberta.encoder.layer[i].attention.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Parameters: 101862402\n"
     ]
    }
   ],
   "source": [
    "print_model_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.embeddings.word_embeddings.weight\n",
      "False\n",
      "roberta.embeddings.position_embeddings.weight\n",
      "False\n",
      "roberta.embeddings.token_type_embeddings.weight\n",
      "False\n",
      "roberta.embeddings.LayerNorm.weight\n",
      "False\n",
      "roberta.embeddings.LayerNorm.bias\n",
      "False\n",
      "roberta.encoder.layer.0.attention.self.query.weight\n",
      "False\n",
      "roberta.encoder.layer.0.attention.self.query.bias\n",
      "False\n",
      "roberta.encoder.layer.0.attention.self.key.weight\n",
      "False\n",
      "roberta.encoder.layer.0.attention.self.key.bias\n",
      "False\n",
      "roberta.encoder.layer.0.attention.self.value.weight\n",
      "False\n",
      "roberta.encoder.layer.0.attention.self.value.bias\n",
      "False\n",
      "roberta.encoder.layer.0.attention.output.dense.weight\n",
      "False\n",
      "roberta.encoder.layer.0.attention.output.dense.bias\n",
      "False\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "False\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "False\n",
      "roberta.encoder.layer.0.intermediateandboom.boom.linear1.weight\n",
      "True\n",
      "roberta.encoder.layer.0.intermediateandboom.boom.linear1.bias\n",
      "True\n",
      "roberta.encoder.layer.0.intermediateandboom.LayerNorm.weight\n",
      "True\n",
      "roberta.encoder.layer.0.intermediateandboom.LayerNorm.bias\n",
      "True\n",
      "roberta.encoder.layer.1.attention.self.query.weight\n",
      "False\n",
      "roberta.encoder.layer.1.attention.self.query.bias\n",
      "False\n",
      "roberta.encoder.layer.1.attention.self.key.weight\n",
      "False\n",
      "roberta.encoder.layer.1.attention.self.key.bias\n",
      "False\n",
      "roberta.encoder.layer.1.attention.self.value.weight\n",
      "False\n",
      "roberta.encoder.layer.1.attention.self.value.bias\n",
      "False\n",
      "roberta.encoder.layer.1.attention.output.dense.weight\n",
      "False\n",
      "roberta.encoder.layer.1.attention.output.dense.bias\n",
      "False\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "False\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "False\n",
      "roberta.encoder.layer.1.intermediateandboom.boom.linear1.weight\n",
      "True\n",
      "roberta.encoder.layer.1.intermediateandboom.boom.linear1.bias\n",
      "True\n",
      "roberta.encoder.layer.1.intermediateandboom.LayerNorm.weight\n",
      "True\n",
      "roberta.encoder.layer.1.intermediateandboom.LayerNorm.bias\n",
      "True\n",
      "roberta.encoder.layer.2.attention.self.query.weight\n",
      "False\n",
      "roberta.encoder.layer.2.attention.self.query.bias\n",
      "False\n",
      "roberta.encoder.layer.2.attention.self.key.weight\n",
      "False\n",
      "roberta.encoder.layer.2.attention.self.key.bias\n",
      "False\n",
      "roberta.encoder.layer.2.attention.self.value.weight\n",
      "False\n",
      "roberta.encoder.layer.2.attention.self.value.bias\n",
      "False\n",
      "roberta.encoder.layer.2.attention.output.dense.weight\n",
      "False\n",
      "roberta.encoder.layer.2.attention.output.dense.bias\n",
      "False\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "False\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "False\n",
      "roberta.encoder.layer.2.intermediateandboom.boom.linear1.weight\n",
      "True\n",
      "roberta.encoder.layer.2.intermediateandboom.boom.linear1.bias\n",
      "True\n",
      "roberta.encoder.layer.2.intermediateandboom.LayerNorm.weight\n",
      "True\n",
      "roberta.encoder.layer.2.intermediateandboom.LayerNorm.bias\n",
      "True\n",
      "roberta.encoder.layer.3.attention.self.query.weight\n",
      "False\n",
      "roberta.encoder.layer.3.attention.self.query.bias\n",
      "False\n",
      "roberta.encoder.layer.3.attention.self.key.weight\n",
      "False\n",
      "roberta.encoder.layer.3.attention.self.key.bias\n",
      "False\n",
      "roberta.encoder.layer.3.attention.self.value.weight\n",
      "False\n",
      "roberta.encoder.layer.3.attention.self.value.bias\n",
      "False\n",
      "roberta.encoder.layer.3.attention.output.dense.weight\n",
      "False\n",
      "roberta.encoder.layer.3.attention.output.dense.bias\n",
      "False\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "False\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "False\n",
      "roberta.encoder.layer.3.intermediateandboom.boom.linear1.weight\n",
      "True\n",
      "roberta.encoder.layer.3.intermediateandboom.boom.linear1.bias\n",
      "True\n",
      "roberta.encoder.layer.3.intermediateandboom.LayerNorm.weight\n",
      "True\n",
      "roberta.encoder.layer.3.intermediateandboom.LayerNorm.bias\n",
      "True\n",
      "roberta.encoder.layer.4.attention.self.query.weight\n",
      "False\n",
      "roberta.encoder.layer.4.attention.self.query.bias\n",
      "False\n",
      "roberta.encoder.layer.4.attention.self.key.weight\n",
      "False\n",
      "roberta.encoder.layer.4.attention.self.key.bias\n",
      "False\n",
      "roberta.encoder.layer.4.attention.self.value.weight\n",
      "False\n",
      "roberta.encoder.layer.4.attention.self.value.bias\n",
      "False\n",
      "roberta.encoder.layer.4.attention.output.dense.weight\n",
      "False\n",
      "roberta.encoder.layer.4.attention.output.dense.bias\n",
      "False\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "False\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "False\n",
      "roberta.encoder.layer.4.intermediateandboom.boom.linear1.weight\n",
      "True\n",
      "roberta.encoder.layer.4.intermediateandboom.boom.linear1.bias\n",
      "True\n",
      "roberta.encoder.layer.4.intermediateandboom.LayerNorm.weight\n",
      "True\n",
      "roberta.encoder.layer.4.intermediateandboom.LayerNorm.bias\n",
      "True\n",
      "roberta.encoder.layer.5.attention.self.query.weight\n",
      "False\n",
      "roberta.encoder.layer.5.attention.self.query.bias\n",
      "False\n",
      "roberta.encoder.layer.5.attention.self.key.weight\n",
      "False\n",
      "roberta.encoder.layer.5.attention.self.key.bias\n",
      "False\n",
      "roberta.encoder.layer.5.attention.self.value.weight\n",
      "False\n",
      "roberta.encoder.layer.5.attention.self.value.bias\n",
      "False\n",
      "roberta.encoder.layer.5.attention.output.dense.weight\n",
      "False\n",
      "roberta.encoder.layer.5.attention.output.dense.bias\n",
      "False\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "False\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "False\n",
      "roberta.encoder.layer.5.intermediateandboom.boom.linear1.weight\n",
      "True\n",
      "roberta.encoder.layer.5.intermediateandboom.boom.linear1.bias\n",
      "True\n",
      "roberta.encoder.layer.5.intermediateandboom.LayerNorm.weight\n",
      "True\n",
      "roberta.encoder.layer.5.intermediateandboom.LayerNorm.bias\n",
      "True\n",
      "roberta.encoder.layer.6.attention.self.query.weight\n",
      "False\n",
      "roberta.encoder.layer.6.attention.self.query.bias\n",
      "False\n",
      "roberta.encoder.layer.6.attention.self.key.weight\n",
      "False\n",
      "roberta.encoder.layer.6.attention.self.key.bias\n",
      "False\n",
      "roberta.encoder.layer.6.attention.self.value.weight\n",
      "False\n",
      "roberta.encoder.layer.6.attention.self.value.bias\n",
      "False\n",
      "roberta.encoder.layer.6.attention.output.dense.weight\n",
      "False\n",
      "roberta.encoder.layer.6.attention.output.dense.bias\n",
      "False\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "False\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "False\n",
      "roberta.encoder.layer.6.intermediateandboom.boom.linear1.weight\n",
      "True\n",
      "roberta.encoder.layer.6.intermediateandboom.boom.linear1.bias\n",
      "True\n",
      "roberta.encoder.layer.6.intermediateandboom.LayerNorm.weight\n",
      "True\n",
      "roberta.encoder.layer.6.intermediateandboom.LayerNorm.bias\n",
      "True\n",
      "roberta.encoder.layer.7.attention.self.query.weight\n",
      "False\n",
      "roberta.encoder.layer.7.attention.self.query.bias\n",
      "False\n",
      "roberta.encoder.layer.7.attention.self.key.weight\n",
      "False\n",
      "roberta.encoder.layer.7.attention.self.key.bias\n",
      "False\n",
      "roberta.encoder.layer.7.attention.self.value.weight\n",
      "False\n",
      "roberta.encoder.layer.7.attention.self.value.bias\n",
      "False\n",
      "roberta.encoder.layer.7.attention.output.dense.weight\n",
      "False\n",
      "roberta.encoder.layer.7.attention.output.dense.bias\n",
      "False\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "False\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "False\n",
      "roberta.encoder.layer.7.intermediateandboom.boom.linear1.weight\n",
      "True\n",
      "roberta.encoder.layer.7.intermediateandboom.boom.linear1.bias\n",
      "True\n",
      "roberta.encoder.layer.7.intermediateandboom.LayerNorm.weight\n",
      "True\n",
      "roberta.encoder.layer.7.intermediateandboom.LayerNorm.bias\n",
      "True\n",
      "roberta.encoder.layer.8.attention.self.query.weight\n",
      "False\n",
      "roberta.encoder.layer.8.attention.self.query.bias\n",
      "False\n",
      "roberta.encoder.layer.8.attention.self.key.weight\n",
      "False\n",
      "roberta.encoder.layer.8.attention.self.key.bias\n",
      "False\n",
      "roberta.encoder.layer.8.attention.self.value.weight\n",
      "False\n",
      "roberta.encoder.layer.8.attention.self.value.bias\n",
      "False\n",
      "roberta.encoder.layer.8.attention.output.dense.weight\n",
      "False\n",
      "roberta.encoder.layer.8.attention.output.dense.bias\n",
      "False\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "False\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "False\n",
      "roberta.encoder.layer.8.intermediateandboom.boom.linear1.weight\n",
      "True\n",
      "roberta.encoder.layer.8.intermediateandboom.boom.linear1.bias\n",
      "True\n",
      "roberta.encoder.layer.8.intermediateandboom.LayerNorm.weight\n",
      "True\n",
      "roberta.encoder.layer.8.intermediateandboom.LayerNorm.bias\n",
      "True\n",
      "roberta.encoder.layer.9.attention.self.query.weight\n",
      "False\n",
      "roberta.encoder.layer.9.attention.self.query.bias\n",
      "False\n",
      "roberta.encoder.layer.9.attention.self.key.weight\n",
      "False\n",
      "roberta.encoder.layer.9.attention.self.key.bias\n",
      "False\n",
      "roberta.encoder.layer.9.attention.self.value.weight\n",
      "False\n",
      "roberta.encoder.layer.9.attention.self.value.bias\n",
      "False\n",
      "roberta.encoder.layer.9.attention.output.dense.weight\n",
      "False\n",
      "roberta.encoder.layer.9.attention.output.dense.bias\n",
      "False\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "False\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "False\n",
      "roberta.encoder.layer.9.intermediateandboom.boom.linear1.weight\n",
      "True\n",
      "roberta.encoder.layer.9.intermediateandboom.boom.linear1.bias\n",
      "True\n",
      "roberta.encoder.layer.9.intermediateandboom.LayerNorm.weight\n",
      "True\n",
      "roberta.encoder.layer.9.intermediateandboom.LayerNorm.bias\n",
      "True\n",
      "roberta.encoder.layer.10.attention.self.query.weight\n",
      "False\n",
      "roberta.encoder.layer.10.attention.self.query.bias\n",
      "False\n",
      "roberta.encoder.layer.10.attention.self.key.weight\n",
      "False\n",
      "roberta.encoder.layer.10.attention.self.key.bias\n",
      "False\n",
      "roberta.encoder.layer.10.attention.self.value.weight\n",
      "False\n",
      "roberta.encoder.layer.10.attention.self.value.bias\n",
      "False\n",
      "roberta.encoder.layer.10.attention.output.dense.weight\n",
      "False\n",
      "roberta.encoder.layer.10.attention.output.dense.bias\n",
      "False\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "False\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "False\n",
      "roberta.encoder.layer.10.intermediateandboom.boom.linear1.weight\n",
      "True\n",
      "roberta.encoder.layer.10.intermediateandboom.boom.linear1.bias\n",
      "True\n",
      "roberta.encoder.layer.10.intermediateandboom.LayerNorm.weight\n",
      "True\n",
      "roberta.encoder.layer.10.intermediateandboom.LayerNorm.bias\n",
      "True\n",
      "roberta.encoder.layer.11.attention.self.query.weight\n",
      "False\n",
      "roberta.encoder.layer.11.attention.self.query.bias\n",
      "False\n",
      "roberta.encoder.layer.11.attention.self.key.weight\n",
      "False\n",
      "roberta.encoder.layer.11.attention.self.key.bias\n",
      "False\n",
      "roberta.encoder.layer.11.attention.self.value.weight\n",
      "False\n",
      "roberta.encoder.layer.11.attention.self.value.bias\n",
      "False\n",
      "roberta.encoder.layer.11.attention.output.dense.weight\n",
      "False\n",
      "roberta.encoder.layer.11.attention.output.dense.bias\n",
      "False\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "False\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "False\n",
      "roberta.encoder.layer.11.intermediateandboom.boom.linear1.weight\n",
      "True\n",
      "roberta.encoder.layer.11.intermediateandboom.boom.linear1.bias\n",
      "True\n",
      "roberta.encoder.layer.11.intermediateandboom.LayerNorm.weight\n",
      "True\n",
      "roberta.encoder.layer.11.intermediateandboom.LayerNorm.bias\n",
      "True\n",
      "roberta.encoder.layer.12.attention.self.query.weight\n",
      "False\n",
      "roberta.encoder.layer.12.attention.self.query.bias\n",
      "False\n",
      "roberta.encoder.layer.12.attention.self.key.weight\n",
      "False\n",
      "roberta.encoder.layer.12.attention.self.key.bias\n",
      "False\n",
      "roberta.encoder.layer.12.attention.self.value.weight\n",
      "False\n",
      "roberta.encoder.layer.12.attention.self.value.bias\n",
      "False\n",
      "roberta.encoder.layer.12.attention.output.dense.weight\n",
      "False\n",
      "roberta.encoder.layer.12.attention.output.dense.bias\n",
      "False\n",
      "roberta.encoder.layer.12.attention.output.LayerNorm.weight\n",
      "False\n",
      "roberta.encoder.layer.12.attention.output.LayerNorm.bias\n",
      "False\n",
      "roberta.encoder.layer.12.intermediateandboom.boom.linear1.weight\n",
      "True\n",
      "roberta.encoder.layer.12.intermediateandboom.boom.linear1.bias\n",
      "True\n",
      "roberta.encoder.layer.12.intermediateandboom.LayerNorm.weight\n",
      "True\n",
      "roberta.encoder.layer.12.intermediateandboom.LayerNorm.bias\n",
      "True\n",
      "roberta.encoder.layer.13.attention.self.query.weight\n",
      "False\n",
      "roberta.encoder.layer.13.attention.self.query.bias\n",
      "False\n",
      "roberta.encoder.layer.13.attention.self.key.weight\n",
      "False\n",
      "roberta.encoder.layer.13.attention.self.key.bias\n",
      "False\n",
      "roberta.encoder.layer.13.attention.self.value.weight\n",
      "False\n",
      "roberta.encoder.layer.13.attention.self.value.bias\n",
      "False\n",
      "roberta.encoder.layer.13.attention.output.dense.weight\n",
      "False\n",
      "roberta.encoder.layer.13.attention.output.dense.bias\n",
      "False\n",
      "roberta.encoder.layer.13.attention.output.LayerNorm.weight\n",
      "False\n",
      "roberta.encoder.layer.13.attention.output.LayerNorm.bias\n",
      "False\n",
      "roberta.encoder.layer.13.intermediateandboom.boom.linear1.weight\n",
      "True\n",
      "roberta.encoder.layer.13.intermediateandboom.boom.linear1.bias\n",
      "True\n",
      "roberta.encoder.layer.13.intermediateandboom.LayerNorm.weight\n",
      "True\n",
      "roberta.encoder.layer.13.intermediateandboom.LayerNorm.bias\n",
      "True\n",
      "roberta.encoder.layer.14.attention.self.query.weight\n",
      "False\n",
      "roberta.encoder.layer.14.attention.self.query.bias\n",
      "False\n",
      "roberta.encoder.layer.14.attention.self.key.weight\n",
      "False\n",
      "roberta.encoder.layer.14.attention.self.key.bias\n",
      "False\n",
      "roberta.encoder.layer.14.attention.self.value.weight\n",
      "False\n",
      "roberta.encoder.layer.14.attention.self.value.bias\n",
      "False\n",
      "roberta.encoder.layer.14.attention.output.dense.weight\n",
      "False\n",
      "roberta.encoder.layer.14.attention.output.dense.bias\n",
      "False\n",
      "roberta.encoder.layer.14.attention.output.LayerNorm.weight\n",
      "False\n",
      "roberta.encoder.layer.14.attention.output.LayerNorm.bias\n",
      "False\n",
      "roberta.encoder.layer.14.intermediateandboom.boom.linear1.weight\n",
      "True\n",
      "roberta.encoder.layer.14.intermediateandboom.boom.linear1.bias\n",
      "True\n",
      "roberta.encoder.layer.14.intermediateandboom.LayerNorm.weight\n",
      "True\n",
      "roberta.encoder.layer.14.intermediateandboom.LayerNorm.bias\n",
      "True\n",
      "roberta.encoder.layer.15.attention.self.query.weight\n",
      "False\n",
      "roberta.encoder.layer.15.attention.self.query.bias\n",
      "False\n",
      "roberta.encoder.layer.15.attention.self.key.weight\n",
      "False\n",
      "roberta.encoder.layer.15.attention.self.key.bias\n",
      "False\n",
      "roberta.encoder.layer.15.attention.self.value.weight\n",
      "False\n",
      "roberta.encoder.layer.15.attention.self.value.bias\n",
      "False\n",
      "roberta.encoder.layer.15.attention.output.dense.weight\n",
      "False\n",
      "roberta.encoder.layer.15.attention.output.dense.bias\n",
      "False\n",
      "roberta.encoder.layer.15.attention.output.LayerNorm.weight\n",
      "False\n",
      "roberta.encoder.layer.15.attention.output.LayerNorm.bias\n",
      "False\n",
      "roberta.encoder.layer.15.intermediateandboom.boom.linear1.weight\n",
      "True\n",
      "roberta.encoder.layer.15.intermediateandboom.boom.linear1.bias\n",
      "True\n",
      "roberta.encoder.layer.15.intermediateandboom.LayerNorm.weight\n",
      "True\n",
      "roberta.encoder.layer.15.intermediateandboom.LayerNorm.bias\n",
      "True\n",
      "roberta.encoder.layer.16.attention.self.query.weight\n",
      "False\n",
      "roberta.encoder.layer.16.attention.self.query.bias\n",
      "False\n",
      "roberta.encoder.layer.16.attention.self.key.weight\n",
      "False\n",
      "roberta.encoder.layer.16.attention.self.key.bias\n",
      "False\n",
      "roberta.encoder.layer.16.attention.self.value.weight\n",
      "False\n",
      "roberta.encoder.layer.16.attention.self.value.bias\n",
      "False\n",
      "roberta.encoder.layer.16.attention.output.dense.weight\n",
      "False\n",
      "roberta.encoder.layer.16.attention.output.dense.bias\n",
      "False\n",
      "roberta.encoder.layer.16.attention.output.LayerNorm.weight\n",
      "False\n",
      "roberta.encoder.layer.16.attention.output.LayerNorm.bias\n",
      "False\n",
      "roberta.encoder.layer.16.intermediateandboom.boom.linear1.weight\n",
      "True\n",
      "roberta.encoder.layer.16.intermediateandboom.boom.linear1.bias\n",
      "True\n",
      "roberta.encoder.layer.16.intermediateandboom.LayerNorm.weight\n",
      "True\n",
      "roberta.encoder.layer.16.intermediateandboom.LayerNorm.bias\n",
      "True\n",
      "roberta.encoder.layer.17.attention.self.query.weight\n",
      "False\n",
      "roberta.encoder.layer.17.attention.self.query.bias\n",
      "False\n",
      "roberta.encoder.layer.17.attention.self.key.weight\n",
      "False\n",
      "roberta.encoder.layer.17.attention.self.key.bias\n",
      "False\n",
      "roberta.encoder.layer.17.attention.self.value.weight\n",
      "False\n",
      "roberta.encoder.layer.17.attention.self.value.bias\n",
      "False\n",
      "roberta.encoder.layer.17.attention.output.dense.weight\n",
      "False\n",
      "roberta.encoder.layer.17.attention.output.dense.bias\n",
      "False\n",
      "roberta.encoder.layer.17.attention.output.LayerNorm.weight\n",
      "False\n",
      "roberta.encoder.layer.17.attention.output.LayerNorm.bias\n",
      "False\n",
      "roberta.encoder.layer.17.intermediateandboom.boom.linear1.weight\n",
      "True\n",
      "roberta.encoder.layer.17.intermediateandboom.boom.linear1.bias\n",
      "True\n",
      "roberta.encoder.layer.17.intermediateandboom.LayerNorm.weight\n",
      "True\n",
      "roberta.encoder.layer.17.intermediateandboom.LayerNorm.bias\n",
      "True\n",
      "roberta.encoder.layer.18.attention.self.query.weight\n",
      "False\n",
      "roberta.encoder.layer.18.attention.self.query.bias\n",
      "False\n",
      "roberta.encoder.layer.18.attention.self.key.weight\n",
      "False\n",
      "roberta.encoder.layer.18.attention.self.key.bias\n",
      "False\n",
      "roberta.encoder.layer.18.attention.self.value.weight\n",
      "False\n",
      "roberta.encoder.layer.18.attention.self.value.bias\n",
      "False\n",
      "roberta.encoder.layer.18.attention.output.dense.weight\n",
      "False\n",
      "roberta.encoder.layer.18.attention.output.dense.bias\n",
      "False\n",
      "roberta.encoder.layer.18.attention.output.LayerNorm.weight\n",
      "False\n",
      "roberta.encoder.layer.18.attention.output.LayerNorm.bias\n",
      "False\n",
      "roberta.encoder.layer.18.intermediateandboom.boom.linear1.weight\n",
      "True\n",
      "roberta.encoder.layer.18.intermediateandboom.boom.linear1.bias\n",
      "True\n",
      "roberta.encoder.layer.18.intermediateandboom.LayerNorm.weight\n",
      "True\n",
      "roberta.encoder.layer.18.intermediateandboom.LayerNorm.bias\n",
      "True\n",
      "roberta.encoder.layer.19.attention.self.query.weight\n",
      "False\n",
      "roberta.encoder.layer.19.attention.self.query.bias\n",
      "False\n",
      "roberta.encoder.layer.19.attention.self.key.weight\n",
      "False\n",
      "roberta.encoder.layer.19.attention.self.key.bias\n",
      "False\n",
      "roberta.encoder.layer.19.attention.self.value.weight\n",
      "False\n",
      "roberta.encoder.layer.19.attention.self.value.bias\n",
      "False\n",
      "roberta.encoder.layer.19.attention.output.dense.weight\n",
      "False\n",
      "roberta.encoder.layer.19.attention.output.dense.bias\n",
      "False\n",
      "roberta.encoder.layer.19.attention.output.LayerNorm.weight\n",
      "False\n",
      "roberta.encoder.layer.19.attention.output.LayerNorm.bias\n",
      "False\n",
      "roberta.encoder.layer.19.intermediateandboom.boom.linear1.weight\n",
      "True\n",
      "roberta.encoder.layer.19.intermediateandboom.boom.linear1.bias\n",
      "True\n",
      "roberta.encoder.layer.19.intermediateandboom.LayerNorm.weight\n",
      "True\n",
      "roberta.encoder.layer.19.intermediateandboom.LayerNorm.bias\n",
      "True\n",
      "roberta.encoder.layer.20.attention.self.query.weight\n",
      "False\n",
      "roberta.encoder.layer.20.attention.self.query.bias\n",
      "False\n",
      "roberta.encoder.layer.20.attention.self.key.weight\n",
      "False\n",
      "roberta.encoder.layer.20.attention.self.key.bias\n",
      "False\n",
      "roberta.encoder.layer.20.attention.self.value.weight\n",
      "False\n",
      "roberta.encoder.layer.20.attention.self.value.bias\n",
      "False\n",
      "roberta.encoder.layer.20.attention.output.dense.weight\n",
      "False\n",
      "roberta.encoder.layer.20.attention.output.dense.bias\n",
      "False\n",
      "roberta.encoder.layer.20.attention.output.LayerNorm.weight\n",
      "False\n",
      "roberta.encoder.layer.20.attention.output.LayerNorm.bias\n",
      "False\n",
      "roberta.encoder.layer.20.intermediateandboom.boom.linear1.weight\n",
      "True\n",
      "roberta.encoder.layer.20.intermediateandboom.boom.linear1.bias\n",
      "True\n",
      "roberta.encoder.layer.20.intermediateandboom.LayerNorm.weight\n",
      "True\n",
      "roberta.encoder.layer.20.intermediateandboom.LayerNorm.bias\n",
      "True\n",
      "roberta.encoder.layer.21.attention.self.query.weight\n",
      "False\n",
      "roberta.encoder.layer.21.attention.self.query.bias\n",
      "False\n",
      "roberta.encoder.layer.21.attention.self.key.weight\n",
      "False\n",
      "roberta.encoder.layer.21.attention.self.key.bias\n",
      "False\n",
      "roberta.encoder.layer.21.attention.self.value.weight\n",
      "False\n",
      "roberta.encoder.layer.21.attention.self.value.bias\n",
      "False\n",
      "roberta.encoder.layer.21.attention.output.dense.weight\n",
      "False\n",
      "roberta.encoder.layer.21.attention.output.dense.bias\n",
      "False\n",
      "roberta.encoder.layer.21.attention.output.LayerNorm.weight\n",
      "False\n",
      "roberta.encoder.layer.21.attention.output.LayerNorm.bias\n",
      "False\n",
      "roberta.encoder.layer.21.intermediateandboom.boom.linear1.weight\n",
      "True\n",
      "roberta.encoder.layer.21.intermediateandboom.boom.linear1.bias\n",
      "True\n",
      "roberta.encoder.layer.21.intermediateandboom.LayerNorm.weight\n",
      "True\n",
      "roberta.encoder.layer.21.intermediateandboom.LayerNorm.bias\n",
      "True\n",
      "roberta.encoder.layer.22.attention.self.query.weight\n",
      "False\n",
      "roberta.encoder.layer.22.attention.self.query.bias\n",
      "False\n",
      "roberta.encoder.layer.22.attention.self.key.weight\n",
      "False\n",
      "roberta.encoder.layer.22.attention.self.key.bias\n",
      "False\n",
      "roberta.encoder.layer.22.attention.self.value.weight\n",
      "False\n",
      "roberta.encoder.layer.22.attention.self.value.bias\n",
      "False\n",
      "roberta.encoder.layer.22.attention.output.dense.weight\n",
      "False\n",
      "roberta.encoder.layer.22.attention.output.dense.bias\n",
      "False\n",
      "roberta.encoder.layer.22.attention.output.LayerNorm.weight\n",
      "False\n",
      "roberta.encoder.layer.22.attention.output.LayerNorm.bias\n",
      "False\n",
      "roberta.encoder.layer.22.intermediateandboom.boom.linear1.weight\n",
      "True\n",
      "roberta.encoder.layer.22.intermediateandboom.boom.linear1.bias\n",
      "True\n",
      "roberta.encoder.layer.22.intermediateandboom.LayerNorm.weight\n",
      "True\n",
      "roberta.encoder.layer.22.intermediateandboom.LayerNorm.bias\n",
      "True\n",
      "roberta.encoder.layer.23.attention.self.query.weight\n",
      "False\n",
      "roberta.encoder.layer.23.attention.self.query.bias\n",
      "False\n",
      "roberta.encoder.layer.23.attention.self.key.weight\n",
      "False\n",
      "roberta.encoder.layer.23.attention.self.key.bias\n",
      "False\n",
      "roberta.encoder.layer.23.attention.self.value.weight\n",
      "False\n",
      "roberta.encoder.layer.23.attention.self.value.bias\n",
      "False\n",
      "roberta.encoder.layer.23.attention.output.dense.weight\n",
      "False\n",
      "roberta.encoder.layer.23.attention.output.dense.bias\n",
      "False\n",
      "roberta.encoder.layer.23.attention.output.LayerNorm.weight\n",
      "False\n",
      "roberta.encoder.layer.23.attention.output.LayerNorm.bias\n",
      "False\n",
      "roberta.encoder.layer.23.intermediateandboom.boom.linear1.weight\n",
      "True\n",
      "roberta.encoder.layer.23.intermediateandboom.boom.linear1.bias\n",
      "True\n",
      "roberta.encoder.layer.23.intermediateandboom.LayerNorm.weight\n",
      "True\n",
      "roberta.encoder.layer.23.intermediateandboom.LayerNorm.bias\n",
      "True\n",
      "classifier.dense.weight\n",
      "True\n",
      "classifier.dense.bias\n",
      "True\n",
      "classifier.out_proj.weight\n",
      "True\n",
      "classifier.out_proj.bias\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Hello, my dog is so cute\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing ModifiedRobertaModel: ['roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.16.output.dense.weight', 'roberta.encoder.layer.12.output.dense.weight', 'roberta.encoder.layer.18.output.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.14.intermediate.dense.weight', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.13.output.dense.weight', 'roberta.encoder.layer.20.output.LayerNorm.weight', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.22.output.LayerNorm.bias', 'roberta.encoder.layer.22.output.dense.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.16.output.LayerNorm.bias', 'roberta.encoder.layer.16.output.dense.bias', 'roberta.encoder.layer.17.output.LayerNorm.weight', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.21.intermediate.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.23.intermediate.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.23.output.LayerNorm.bias', 'roberta.encoder.layer.15.intermediate.dense.weight', 'roberta.encoder.layer.19.intermediate.dense.weight', 'roberta.encoder.layer.13.intermediate.dense.bias', 'roberta.encoder.layer.23.output.LayerNorm.weight', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.12.intermediate.dense.bias', 'roberta.encoder.layer.17.output.dense.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.13.output.dense.bias', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.20.intermediate.dense.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.2.output.dense.weight', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.14.output.dense.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.17.intermediate.dense.bias', 'lm_head.bias', 'roberta.encoder.layer.15.output.LayerNorm.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.23.output.dense.weight', 'roberta.encoder.layer.12.output.dense.bias', 'roberta.encoder.layer.16.output.LayerNorm.weight', 'roberta.encoder.layer.14.output.LayerNorm.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.19.output.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.12.intermediate.dense.weight', 'lm_head.dense.bias', 'roberta.encoder.layer.15.intermediate.dense.bias', 'roberta.encoder.layer.22.intermediate.dense.weight', 'roberta.encoder.layer.14.output.dense.bias', 'roberta.encoder.layer.13.output.LayerNorm.bias', 'roberta.encoder.layer.17.intermediate.dense.weight', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.15.output.dense.weight', 'roberta.encoder.layer.19.output.dense.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.14.intermediate.dense.bias', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.20.output.dense.bias', 'roberta.encoder.layer.16.intermediate.dense.bias', 'roberta.encoder.layer.22.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.18.intermediate.dense.bias', 'roberta.encoder.layer.21.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.15.output.LayerNorm.bias', 'roberta.encoder.layer.17.output.LayerNorm.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.22.intermediate.dense.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.13.intermediate.dense.weight', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.18.output.LayerNorm.bias', 'roberta.encoder.layer.21.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.18.intermediate.dense.weight', 'roberta.encoder.layer.21.output.dense.weight', 'roberta.encoder.layer.21.intermediate.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.20.intermediate.dense.weight', 'roberta.encoder.layer.23.intermediate.dense.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.22.output.dense.bias', 'roberta.encoder.layer.13.output.LayerNorm.weight', 'roberta.encoder.layer.12.output.LayerNorm.weight', 'roberta.encoder.layer.21.output.dense.bias', 'roberta.encoder.layer.20.output.dense.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.19.intermediate.dense.bias', 'roberta.encoder.layer.18.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.17.output.dense.bias', 'lm_head.decoder.weight', 'roberta.encoder.layer.12.output.LayerNorm.bias', 'roberta.encoder.layer.23.output.dense.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.15.output.dense.bias', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.16.intermediate.dense.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.18.output.dense.bias', 'roberta.encoder.layer.14.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.19.output.LayerNorm.bias', 'roberta.encoder.layer.19.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.20.output.LayerNorm.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing ModifiedRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ModifiedRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ModifiedRobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.encoder.layer.11.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.23.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.21.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.15.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.10.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.16.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.5.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.10.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.4.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.15.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.18.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.5.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.3.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.19.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.19.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.13.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.14.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.17.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.3.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.22.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.10.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.1.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.15.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.8.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.5.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.21.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.7.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.18.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.12.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.8.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.9.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.11.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.11.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.0.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.5.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.1.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.3.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.21.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.20.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.22.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.1.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.14.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.10.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.23.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.6.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.7.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.0.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.3.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.23.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.14.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.13.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.6.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.6.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.17.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.7.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.21.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.12.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.17.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.4.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.12.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.9.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.4.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.2.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.15.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.16.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.2.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.17.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.12.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.6.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.8.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.20.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.18.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.7.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.14.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.22.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.13.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.19.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.2.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.9.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.1.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.11.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.8.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.20.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.20.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.22.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.18.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.4.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.9.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.2.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.0.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.16.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.23.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.13.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.16.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.0.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.19.intermediateandboom.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer#, RobertaModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
    "model = ModifiedRobertaModel.from_pretrained(\"roberta-large\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-emotion were not used when initializing ModifiedRobertaForSequenceClassification: ['roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight']\n",
      "- This IS expected if you are initializing ModifiedRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ModifiedRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ModifiedRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion and are newly initialized: ['roberta.encoder.layer.11.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.8.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.3.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.5.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.7.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.6.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.7.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.6.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.10.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.7.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.8.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.9.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.2.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.5.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.11.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.4.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.9.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.1.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.11.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.0.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.10.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.9.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.4.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.4.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.11.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.5.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.1.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.3.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.2.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.8.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.1.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.3.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.5.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.4.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.9.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.2.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.2.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.0.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.10.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.6.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.6.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.7.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.3.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.8.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.0.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.10.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.1.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.0.intermediateandboom.LayerNorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-emotion were not used when initializing ModifiedRobertaForSequenceClassification: ['roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight']\n",
      "- This IS expected if you are initializing ModifiedRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ModifiedRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ModifiedRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion and are newly initialized: ['roberta.encoder.layer.11.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.8.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.3.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.5.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.7.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.6.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.7.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.6.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.10.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.7.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.8.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.9.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.2.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.5.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.11.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.4.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.9.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.1.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.11.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.0.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.10.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.9.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.4.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.4.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.11.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.5.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.1.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.3.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.2.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.8.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.1.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.3.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.5.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.4.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.9.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.2.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.2.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.0.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.10.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.6.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.6.intermediateandboom.LayerNorm.weight', 'roberta.encoder.layer.7.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.3.intermediateandboom.boom.linear1.bias', 'roberta.encoder.layer.8.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.0.intermediateandboom.boom.linear1.weight', 'roberta.encoder.layer.10.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.1.intermediateandboom.LayerNorm.bias', 'roberta.encoder.layer.0.intermediateandboom.LayerNorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer#, RobertaForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-emotion\")\n",
    "model = ModifiedRobertaForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-emotion\", problem_type=\"multi_label_classification\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is so cute\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n",
    "\n",
    "# To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
    "num_labels = len(model.config.id2label)\n",
    "model = ModifiedRobertaForSequenceClassification.from_pretrained(\n",
    "    \"cardiffnlp/twitter-roberta-base-emotion\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "labels = torch.sum(\n",
    "    torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n",
    ").to(torch.float)\n",
    "loss = model(**inputs, labels=labels).loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6074, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
